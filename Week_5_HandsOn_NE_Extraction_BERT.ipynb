{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Extraction with BERT\n",
        "\n",
        "## What is BERT?\n",
        "\n",
        "Read the paper: https://arxiv.org/pdf/1810.04805\n",
        "\n",
        "Read this blog to understand transformers: https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "## CoNLL dataset\n",
        "\n",
        "Read more about it here: https://www.clips.uantwerpen.be/conll2003/ner/"
      ],
      "metadata": {
        "id": "hLm7LhbZQt0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets"
      ],
      "metadata": {
        "id": "7UyHioHsgfRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eee428a-269b-428f-a983-a5c347bbe2fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "gOtCaMtIg9_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942fa496-f4e1-44ab-c3a1-d697629de933"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's inspect the dataset\n",
        "sentence_0 = dataset[\"train\"][0]\n",
        "print(sentence_0)"
      ],
      "metadata": {
        "id": "jZbACIbQg90Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5848dc4-cec5-4204-bcc3-375ad1e0842e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you identify each of the keys?\n",
        "\n",
        "Explain:\n",
        "- id\n",
        "- tokens\n",
        "- pos_tags\n",
        "- chunk_tags\n",
        "- ner_tags\n",
        "\n"
      ],
      "metadata": {
        "id": "baazFSErhecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_0_str = \" \".join(dataset[\"train\"][0]['tokens'])\n",
        "sentence_0_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "juGBYZcshrMO",
        "outputId": "9f23e639-6a59-4d11-da69-5cea02159be7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EU rejects German call to boycott British lamb .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map pos tag numbers to their labels\n",
        "pos_tags = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
        "chunk_tags = dataset[\"train\"].features[\"chunk_tags\"].feature.names\n",
        "ner_tags = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "print(pos_tags)\n",
        "print(chunk_tags)\n",
        "print(ner_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12WJQLzbiA05",
        "outputId": "72dd0b91-69da-4e4e-a862-67f155c89696"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
            "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n",
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POS Tags for sentence_0\")\n",
        "for i, pos_ in enumerate(sentence_0['pos_tags']):\n",
        "    print(f\"{pos_}  \\t- {pos_tags[pos_]} \\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtu51lx8jDCH",
        "outputId": "9a915dbc-aea2-4cbb-89ce-4a8f42bb510f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags for sentence_0\n",
            "22  \t- NNP \t- EU\n",
            "42  \t- VBZ \t- rejects\n",
            "16  \t- JJ \t- German\n",
            "21  \t- NN \t- call\n",
            "35  \t- TO \t- to\n",
            "37  \t- VB \t- boycott\n",
            "16  \t- JJ \t- British\n",
            "21  \t- NN \t- lamb\n",
            "7  \t- . \t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chunk Tags for sentence_0\")\n",
        "for i, chunk_ in enumerate(sentence_0['chunk_tags']):\n",
        "    print(f\"{chunk_} \\t- {chunk_tags[chunk_]} \\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47xQckc8kIY7",
        "outputId": "5f281f92-42a2-43a3-9d22-85882b57b1f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Tags for sentence_0\n",
            "11 \t- B-NP \t- EU\n",
            "21 \t- B-VP \t- rejects\n",
            "11 \t- B-NP \t- German\n",
            "12 \t- I-NP \t- call\n",
            "21 \t- B-VP \t- to\n",
            "22 \t- I-VP \t- boycott\n",
            "11 \t- B-NP \t- British\n",
            "12 \t- I-NP \t- lamb\n",
            "0 \t- O \t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NER Tags for sentence_0\")\n",
        "for i, ner_ in enumerate(sentence_0['ner_tags']):\n",
        "    print(f\"{ner_} \\t- {ner_tags[ner_]} \\t\\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxBVp0VWkSiI",
        "outputId": "d0d3c898-9827-41f9-9005-ed1c2f52dab0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Tags for sentence_0\n",
            "3 \t- B-ORG \t\t- EU\n",
            "0 \t- O \t\t- rejects\n",
            "7 \t- B-MISC \t\t- German\n",
            "0 \t- O \t\t- call\n",
            "0 \t- O \t\t- to\n",
            "0 \t- O \t\t- boycott\n",
            "7 \t- B-MISC \t\t- British\n",
            "0 \t- O \t\t- lamb\n",
            "0 \t- O \t\t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the dataset\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Validation size: {len(dataset['validation'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg37XUTXlctD",
        "outputId": "4e73f786-43f8-4e63-9efa-910269a81b30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 14041\n",
            "Validation size: 3250\n",
            "Test size: 3453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation for traning"
      ],
      "metadata": {
        "id": "y8WGS7Oalvzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the model"
      ],
      "metadata": {
        "id": "rFxVJfP3xOL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = BertForTokenClassification.from_pretrained(\n",
        "            'bert-base-cased',\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "I_FW_GkigvuZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the traning loop"
      ],
      "metadata": {
        "id": "fANj46xDyKdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=3):\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Early Stopping parameters\n",
        "    patience = 2\n",
        "    best_eval_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss  # Access the loss from the model output\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate\n",
        "\n",
        "        # Calculate the average training loss\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        total_eval_loss = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for validation\n",
        "            for batch in eval_dataloader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass (no backpropagation in evaluation mode)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "        # Calculate the average validation loss\n",
        "        eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "        print(f\"Validation loss: {eval_loss:.4f}\")\n",
        "\n",
        "        # Early Stopping Check\n",
        "        if eval_loss < best_eval_loss:\n",
        "            best_eval_loss = eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "            print(\"Model improved, saving best model...\")\n",
        "            model.save_pretrained(\"best_model\")\n",
        "            tokenizer.save_pretrained(\"best_model\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Return to training mode after evaluation\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "CkmsKEJ3x3nB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the evaluation function"
      ],
      "metadata": {
        "id": "024hkY9gydgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install seqeval"
      ],
      "metadata": {
        "id": "GcfEsdLgzZx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b9f83b-9ff0-4ce8-c67d-72461b09e7fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, test_dataloader, id2label, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for batch in test_dataloader:\n",
        "            # Move input data to the device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)  # True labels for each token\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Raw prediction scores\n",
        "\n",
        "            # Convert logits to label indices\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Process each sentence in the batch\n",
        "            for i in range(len(labels)):\n",
        "                # Extract the true labels and predictions for each token\n",
        "                true_label_ids = labels[i]\n",
        "                pred_label_ids = predictions[i]\n",
        "\n",
        "                # Convert label indices to label names\n",
        "                true_label = [id2label[label_id.item()] for label_id in true_label_ids if label_id != -100]\n",
        "                pred_label = [id2label[pred_id.item()] for pred_id, label_id in zip(pred_label_ids, true_label_ids) if label_id != -100]\n",
        "\n",
        "                # Append to the lists for evaluation\n",
        "                true_labels.append(true_label)\n",
        "                predicted_labels.append(pred_label)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(true_labels, predicted_labels))\n"
      ],
      "metadata": {
        "id": "RchFco08yky1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset):\n",
        "        self.tokenized_dataset = tokenized_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenized_dataset[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def prepare_data():\n",
        "    # Load the CoNLL-2003 dataset\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Get label list from dataset\n",
        "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"],\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    # Tokenize datasets\n",
        "    tokenized_datasets = dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    return tokenized_datasets, len(label_list)\n"
      ],
      "metadata": {
        "id": "Eeykd69ggUzu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the main() function"
      ],
      "metadata": {
        "id": "QFeiqJbi2k_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    print(\"Loading and preparing dataset...\")\n",
        "    tokenized_datasets, num_labels = prepare_data()\n",
        "\n",
        "    # Convert to custom Dataset objects\n",
        "    train_dataset = NERDataset(tokenized_datasets[\"train\"])\n",
        "    eval_dataset = NERDataset(tokenized_datasets[\"validation\"])\n",
        "    test_dataset = NERDataset(tokenized_datasets[\"test\"])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,  # Increased batch size\n",
        "        shuffle=True\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=32\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    model = NERModel(num_labels=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=9e-5,\n",
        "        weight_decay=0.02,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    num_epochs = 3\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    num_warmup_steps = num_training_steps // 10\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    evaluate_model(model, eval_dataloader, id2label, device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JQ5EYOtRzXK9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcCdZiTJ2jYS",
        "outputId": "08c8e192-66b3-4d89-c8ab-bb7549c55712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading and preparing dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement inference function\n"
      ],
      "metadata": {
        "id": "gEN8k-e1CxgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def prepare_inference(model_path=None):\n",
        "    \"\"\"Initialize tokenizer and load model for inference\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Load trained model if path provided, otherwise use existing model\n",
        "    if model_path:\n",
        "        model = torch.load(model_path)\n",
        "\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    return tokenizer, id2label\n",
        "\n",
        "def inference(text, model, tokenizer, id2label, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform NER inference on input text\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        model: Trained NER model\n",
        "        tokenizer: BERT tokenizer\n",
        "        id2label (dict): Mapping from label ids to label names\n",
        "        device (str): Device to run inference on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (word, entity_label)\n",
        "    \"\"\"\n",
        "    ## fill in your code\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the text\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", is_split_into_words=False, truncation=True)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Convert predictions to labels\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "    labels = [id2label[label_id] for label_id in predictions.squeeze().tolist()]\n",
        "\n",
        "    # Align predictions with words\n",
        "    word_ids = encoding.word_ids()\n",
        "    labeled_words = []\n",
        "    current_word = \"\"\n",
        "    current_label = None\n",
        "\n",
        "    for idx, word_id in enumerate(word_ids):\n",
        "        if word_id is None:\n",
        "            continue  # Skip special tokens ([CLS], [SEP])\n",
        "\n",
        "        token = tokens[idx]\n",
        "        label = labels[idx]\n",
        "\n",
        "        if word_id != current_word and current_word is not None:\n",
        "            labeled_words.append((current_word, current_label))\n",
        "            current_word = \"\"\n",
        "            current_label = None\n",
        "\n",
        "        if token.startswith(\"##\"):\n",
        "            # Handle subword tokens\n",
        "            current_word += token[2:]\n",
        "        else:\n",
        "            # New word\n",
        "            if current_word:\n",
        "                labeled_words.append((current_word, current_label))\n",
        "            current_word = token\n",
        "            current_label = label\n",
        "\n",
        "    # Add the last word\n",
        "    if current_word:\n",
        "        labeled_words.append((current_word, current_label))\n",
        "\n",
        "    return labeled_words\n",
        "\n",
        "def print_entities(labeled_words):\n",
        "    \"\"\"Pretty print the labeled entities\"\"\"\n",
        "    current_entity = None\n",
        "    entity_text = []\n",
        "\n",
        "    for word, label in labeled_words:\n",
        "        if label == \"O\":\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = None\n",
        "                entity_text = []\n",
        "        elif label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "            current_entity = label[2:]  # Remove \"B-\" prefix\n",
        "            entity_text = [word]\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity == label[2:]:  # If it's the same entity type\n",
        "                entity_text.append(word)\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = label[2:]\n",
        "                entity_text = [word]\n",
        "\n",
        "    if current_entity:  # Print last entity if exists\n",
        "        print(f\"{current_entity}: {' '.join(entity_text)}\")"
      ],
      "metadata": {
        "id": "0WZwPk5p22hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First initialize\n",
        "tokenizer, id2label = prepare_inference()\n",
        "\n",
        "# Example texts to analyze\n",
        "texts = [\n",
        "    \"John Smith works at Microsoft in Seattle and visited New York last summer.\",\n",
        "    \"The European Union signed a trade deal with Japan in Brussels.\",\n",
        "    \"Tesla CEO Elon Musk announced new features coming to their vehicles.\"\n",
        "]\n",
        "\n",
        "# Process each text\n",
        "for text in texts:\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Entities found:\")\n",
        "    results = inference(text, model, tokenizer, id2label)\n",
        "    print_entities(results)"
      ],
      "metadata": {
        "id": "jwooWAI3Cm1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lbPrensoC2Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1bd58a-86f7-49b6-be8b-e7bce3b8fbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/BERT_NER/bert_ner_model.pth\"\n",
        "torch.save(model, save_path)"
      ],
      "metadata": {
        "id": "KE-eEtq1NDXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B80Wb7wENccP",
        "outputId": "ccf33684-5fce-45c1-e299-e0e5965e45be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f6cbf9c9768c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(save_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyjSGG4YNtio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
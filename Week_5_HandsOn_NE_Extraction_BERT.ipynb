{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27328a2deac846699666d51d70f81538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be6a01aed0e944ca951d2dd819532490",
              "IPY_MODEL_cfb993b194eb4971a32662c83b414e4e",
              "IPY_MODEL_5ff48b89a29a415e94d584b2a5c8a837"
            ],
            "layout": "IPY_MODEL_a264a1c9500b49cba38f8d29d4388c03"
          }
        },
        "be6a01aed0e944ca951d2dd819532490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4eb4b505fb0401eab7c12f02fae6ebd",
            "placeholder": "​",
            "style": "IPY_MODEL_33e1e7b39e124fdeb3f565651168c425",
            "value": "Map: 100%"
          }
        },
        "cfb993b194eb4971a32662c83b414e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f60060419bb418ab5e138712e5c8d10",
            "max": 3453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d648c668b98648138032f85245e1cad5",
            "value": 3453
          }
        },
        "5ff48b89a29a415e94d584b2a5c8a837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22be373f64e34651875718dc52a40c42",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4a11b8d0094e579a00dac2763b0d20",
            "value": " 3453/3453 [00:02&lt;00:00, 1549.70 examples/s]"
          }
        },
        "a264a1c9500b49cba38f8d29d4388c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4eb4b505fb0401eab7c12f02fae6ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33e1e7b39e124fdeb3f565651168c425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f60060419bb418ab5e138712e5c8d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d648c668b98648138032f85245e1cad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22be373f64e34651875718dc52a40c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4a11b8d0094e579a00dac2763b0d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Extraction with BERT\n",
        "\n",
        "## What is BERT?\n",
        "\n",
        "Read the paper: https://arxiv.org/pdf/1810.04805\n",
        "\n",
        "Read this blog to understand transformers: https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "## CoNLL dataset\n",
        "\n",
        "Read more about it here: https://www.clips.uantwerpen.be/conll2003/ner/"
      ],
      "metadata": {
        "id": "hLm7LhbZQt0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets"
      ],
      "metadata": {
        "id": "7UyHioHsgfRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "gOtCaMtIg9_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's inspect the dataset\n",
        "sentence_0 = dataset[\"train\"][0]\n",
        "print(sentence_0)"
      ],
      "metadata": {
        "id": "jZbACIbQg90Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918848db-6cac-47fa-be25-8ecdbf0e55ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you identify each of the keys?\n",
        "\n",
        "Explain:\n",
        "- id\n",
        "- tokens\n",
        "- pos_tags\n",
        "- chunk_tags\n",
        "- ner_tags\n",
        "\n"
      ],
      "metadata": {
        "id": "baazFSErhecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_0_str = \" \".join(dataset[\"train\"][0]['tokens'])\n",
        "sentence_0_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "juGBYZcshrMO",
        "outputId": "0b8c9ad3-05f3-40c6-aac8-c5c15028ecd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EU rejects German call to boycott British lamb .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map pos tag numbers to their labels\n",
        "pos_tags = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
        "chunk_tags = dataset[\"train\"].features[\"chunk_tags\"].feature.names\n",
        "ner_tags = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "print(pos_tags)\n",
        "print(chunk_tags)\n",
        "print(ner_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12WJQLzbiA05",
        "outputId": "a116b2d6-17e6-40ca-d2bf-b85a6516ac29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
            "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n",
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POS Tags for sentence_0\")\n",
        "for i, pos_ in enumerate(sentence_0['pos_tags']):\n",
        "    print(f\"{pos_}  \\t- {pos_tags[pos_]} \\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtu51lx8jDCH",
        "outputId": "bd1af1b0-88d8-44f4-e33e-7ff2287d7edb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags for sentence_0\n",
            "22  \t- NNP \t- EU\n",
            "42  \t- VBZ \t- rejects\n",
            "16  \t- JJ \t- German\n",
            "21  \t- NN \t- call\n",
            "35  \t- TO \t- to\n",
            "37  \t- VB \t- boycott\n",
            "16  \t- JJ \t- British\n",
            "21  \t- NN \t- lamb\n",
            "7  \t- . \t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chunk Tags for sentence_0\")\n",
        "for i, chunk_ in enumerate(sentence_0['chunk_tags']):\n",
        "    print(f\"{chunk_} \\t- {chunk_tags[chunk_]} \\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47xQckc8kIY7",
        "outputId": "68cb88b0-0a9e-4634-eab8-7fe795e4ec64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Tags for sentence_0\n",
            "11 \t- B-NP \t- EU\n",
            "21 \t- B-VP \t- rejects\n",
            "11 \t- B-NP \t- German\n",
            "12 \t- I-NP \t- call\n",
            "21 \t- B-VP \t- to\n",
            "22 \t- I-VP \t- boycott\n",
            "11 \t- B-NP \t- British\n",
            "12 \t- I-NP \t- lamb\n",
            "0 \t- O \t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NER Tags for sentence_0\")\n",
        "for i, ner_ in enumerate(sentence_0['ner_tags']):\n",
        "    print(f\"{ner_} \\t- {ner_tags[ner_]} \\t\\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxBVp0VWkSiI",
        "outputId": "7497e304-3804-460a-cfeb-d006fad32ecb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Tags for sentence_0\n",
            "3 \t- B-ORG \t\t- EU\n",
            "0 \t- O \t\t- rejects\n",
            "7 \t- B-MISC \t\t- German\n",
            "0 \t- O \t\t- call\n",
            "0 \t- O \t\t- to\n",
            "0 \t- O \t\t- boycott\n",
            "7 \t- B-MISC \t\t- British\n",
            "0 \t- O \t\t- lamb\n",
            "0 \t- O \t\t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the dataset\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Validation size: {len(dataset['validation'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg37XUTXlctD",
        "outputId": "11f8b2ad-f368-4864-be38-3ed590a256e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 14041\n",
            "Validation size: 3250\n",
            "Test size: 3453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation for traning"
      ],
      "metadata": {
        "id": "y8WGS7Oalvzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the model"
      ],
      "metadata": {
        "id": "rFxVJfP3xOL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset):\n",
        "        self.tokenized_dataset = tokenized_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenized_dataset[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def prepare_data():\n",
        "    # Load the CoNLL-2003 dataset\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Get label list from dataset\n",
        "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"],\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    # Tokenize datasets\n",
        "    tokenized_datasets = dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    return tokenized_datasets, len(label_list)\n"
      ],
      "metadata": {
        "id": "Eeykd69ggUzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = BertForTokenClassification.from_pretrained(\n",
        "            'bert-base-cased',\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "I_FW_GkigvuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the traning loop"
      ],
      "metadata": {
        "id": "fANj46xDyKdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=3):\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Early Stopping parameters\n",
        "    patience = 2\n",
        "    best_eval_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss  # Access the loss from the model output\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate\n",
        "\n",
        "        # Calculate the average training loss\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        total_eval_loss = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for validation\n",
        "            for batch in eval_dataloader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass (no backpropagation in evaluation mode)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "        # Calculate the average validation loss\n",
        "        eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "        print(f\"Validation loss: {eval_loss:.4f}\")\n",
        "\n",
        "        # Early Stopping Check\n",
        "        if eval_loss < best_eval_loss:\n",
        "            best_eval_loss = eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "            print(\"Model improved, saving best model...\")\n",
        "            model.save_pretrained(\"best_model\")\n",
        "            tokenizer.save_pretrained(\"best_model\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Return to training mode after evaluation\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "CkmsKEJ3x3nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the evaluation function"
      ],
      "metadata": {
        "id": "024hkY9gydgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install seqeval"
      ],
      "metadata": {
        "id": "GcfEsdLgzZx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, test_dataloader, id2label, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for batch in test_dataloader:\n",
        "            # Move input data to the device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)  # True labels for each token\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Raw prediction scores\n",
        "\n",
        "            # Convert logits to label indices\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Process each sentence in the batch\n",
        "            for i in range(len(labels)):\n",
        "                # Extract the true labels and predictions for each token\n",
        "                true_label_ids = labels[i]\n",
        "                pred_label_ids = predictions[i]\n",
        "\n",
        "                # Convert label indices to label names\n",
        "                true_label = [id2label[label_id.item()] for label_id in true_label_ids if label_id != -100]\n",
        "                pred_label = [id2label[pred_id.item()] for pred_id, label_id in zip(pred_label_ids, true_label_ids) if label_id != -100]\n",
        "\n",
        "                # Append to the lists for evaluation\n",
        "                true_labels.append(true_label)\n",
        "                predicted_labels.append(pred_label)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(true_labels, predicted_labels))\n"
      ],
      "metadata": {
        "id": "RchFco08yky1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the main() function"
      ],
      "metadata": {
        "id": "QFeiqJbi2k_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    print(\"Loading and preparing dataset...\")\n",
        "    tokenized_datasets, num_labels = prepare_data()\n",
        "\n",
        "    # Convert to custom Dataset objects\n",
        "    train_dataset = NERDataset(tokenized_datasets[\"train\"])\n",
        "    eval_dataset = NERDataset(tokenized_datasets[\"validation\"])\n",
        "    test_dataset = NERDataset(tokenized_datasets[\"test\"])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,  # Increased batch size\n",
        "        shuffle=True\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=32\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    # Model initialization\n",
        "    model = NERModel(num_labels=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer with better parameters\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=9e-5,              # Reduced learning rate\n",
        "        weight_decay=0.02,    # Increased weight decay for regularization\n",
        "        eps=1e-8              # Stability parameter\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    num_epochs = 3\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    num_warmup_steps = num_training_steps // 10\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    evaluate_model(model, eval_dataloader, id2label, device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JQ5EYOtRzXK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "27328a2deac846699666d51d70f81538",
            "be6a01aed0e944ca951d2dd819532490",
            "cfb993b194eb4971a32662c83b414e4e",
            "5ff48b89a29a415e94d584b2a5c8a837",
            "a264a1c9500b49cba38f8d29d4388c03",
            "e4eb4b505fb0401eab7c12f02fae6ebd",
            "33e1e7b39e124fdeb3f565651168c425",
            "9f60060419bb418ab5e138712e5c8d10",
            "d648c668b98648138032f85245e1cad5",
            "22be373f64e34651875718dc52a40c42",
            "bb4a11b8d0094e579a00dac2763b0d20"
          ]
        },
        "id": "PcCdZiTJ2jYS",
        "outputId": "23c5aadb-d4b1-4557-8ef5-920859c0b1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27328a2deac846699666d51d70f81538"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement inference function\n"
      ],
      "metadata": {
        "id": "gEN8k-e1CxgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def prepare_inference(model_path=None):\n",
        "    \"\"\"Initialize tokenizer and load model for inference\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Load trained model if path provided, otherwise use existing model\n",
        "    if model_path:\n",
        "        model = torch.load(model_path)\n",
        "\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    return tokenizer, id2label\n",
        "\n",
        "def inference(text, model, tokenizer, id2label, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform NER inference on input text\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        model: Trained NER model\n",
        "        tokenizer: BERT tokenizer\n",
        "        id2label (dict): Mapping from label ids to label names\n",
        "        device (str): Device to run inference on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (word, entity_label)\n",
        "    \"\"\"\n",
        "    ## fill in your code\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "\n",
        "\n",
        "    # Tokenize the text\n",
        "\n",
        "    # Perform inference\n",
        "\n",
        "\n",
        "    # Convert predictions to labels\n",
        "\n",
        "\n",
        "    # Align predictions with words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Combine words with their predicted labels\n",
        "\n",
        "\n",
        "    return labeled_words\n",
        "\n",
        "def print_entities(labeled_words):\n",
        "    \"\"\"Pretty print the labeled entities\"\"\"\n",
        "    current_entity = None\n",
        "    entity_text = []\n",
        "\n",
        "    for word, label in labeled_words:\n",
        "        if label == \"O\":\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = None\n",
        "                entity_text = []\n",
        "        elif label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "            current_entity = label[2:]  # Remove \"B-\" prefix\n",
        "            entity_text = [word]\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity == label[2:]:  # If it's the same entity type\n",
        "                entity_text.append(word)\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = label[2:]\n",
        "                entity_text = [word]\n",
        "\n",
        "    if current_entity:  # Print last entity if exists\n",
        "        print(f\"{current_entity}: {' '.join(entity_text)}\")"
      ],
      "metadata": {
        "id": "0WZwPk5p22hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First initialize\n",
        "tokenizer, id2label = prepare_inference()\n",
        "\n",
        "# Example texts to analyze\n",
        "texts = [\n",
        "    \"John Smith works at Microsoft in Seattle and visited New York last summer.\",\n",
        "    \"The European Union signed a trade deal with Japan in Brussels.\",\n",
        "    \"Tesla CEO Elon Musk announced new features coming to their vehicles.\"\n",
        "]\n",
        "\n",
        "# Process each text\n",
        "for text in texts:\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Entities found:\")\n",
        "    results = inference(text, model, tokenizer, id2label)\n",
        "    print_entities(results)"
      ],
      "metadata": {
        "id": "jwooWAI3Cm1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "lbPrensoC2Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1bd58a-86f7-49b6-be8b-e7bce3b8fbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/BERT_NER/bert_ner_model.pth\"\n",
        "torch.save(model, save_path)"
      ],
      "metadata": {
        "id": "KE-eEtq1NDXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B80Wb7wENccP",
        "outputId": "ccf33684-5fce-45c1-e299-e0e5965e45be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f6cbf9c9768c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(save_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyjSGG4YNtio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}